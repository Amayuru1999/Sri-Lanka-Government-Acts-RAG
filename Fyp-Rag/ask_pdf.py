
import logging
import os
import pickle
from dotenv import load_dotenv

# from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA


from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# Re-ranking
from sentence_transformers import CrossEncoder

logging.basicConfig(level=logging.INFO)


#  Load or initialize embeddings

def load_or_initialize_embeddings():
    if os.path.exists('legal_bert_embeddings.pkl'):
        logging.info("Loading cached embeddings...")
        with open('legal_bert_embeddings.pkl', 'rb') as f:
            embeddings = pickle.load(f)
        return embeddings
    else:
        logging.info("Initializing new LEGAL-BERT embeddings...")
        embeddings = HuggingFaceEmbeddings(model_name="nlpaueb/legal-bert-base-uncased")
        with open('legal_bert_embeddings.pkl', 'wb') as f:
            pickle.dump(embeddings, f)
        return embeddings


#  Cross-encoder re-ranking
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank(query, docs):
    """Re-rank retrieved documents by semantic relevance."""
    pairs = [[query, d.page_content] for d in docs]
    scores = cross_encoder.predict(pairs)
    sorted_docs = [doc for _, doc in sorted(zip(scores, docs), reverse=True)]
    return sorted_docs


#  Initialize  RAG system
def initialize_rag_system():
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logging.error("OPENAI_API_KEY not found in .env")
        return None

    embeddings = load_or_initialize_embeddings()

    # Connect to ChromaDB
    persist_directory = "./civil_db"
    if not os.path.exists(persist_directory):
        logging.error("ChromaDB not found!")
        return None

    vector_db = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings,
        collection_name="civil_docs",
    )

    if vector_db._collection.count() == 0:
        logging.error("No documents found in ChromaDB!")
        return None

    logging.info(f"Found {vector_db._collection.count()} documents in ChromaDB")

    # Load all docs for BM25 keyword search
    logging.info("Loading documents for BM25 keyword search...")
    all_docs = vector_db.similarity_search("placeholder", k=vector_db._collection.count())
    bm25_retriever = BM25Retriever.from_documents(all_docs)
    bm25_retriever.k = 5

    # Dense retriever
    dense_retriever = vector_db.as_retriever(search_kwargs={"k": 5})

    # Hybrid retrieval
    hybrid_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, dense_retriever],
        weights=[0.5, 0.5]
    )

    # Multi-query retrieval
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, max_tokens=1024)
    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=hybrid_retriever,
        llm=llm
    )

    # Contextual compression
    compressor = LLMChainExtractor.from_llm(llm)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=multi_query_retriever
    )

    # Prompt with citations
    prompt_template = """
    You are a legal assistant specialized in Sri Lankan Government Acts. 
    
    If the question is about legal matters and you have relevant context from the provided legal documents, use ONLY those documents to answer. Include source citations in square brackets after each relevant fact.
    
    If the question is a casual greeting (like "Hi", "Hello", "How are you?") or not related to legal matters, respond politely and guide the user to ask about Sri Lankan Government Acts.
    
    If the question is legal but you don't have enough context from the provided documents, say: "I don't have enough information in the available legal documents to answer this question."

    Context:
    {context}

    Question:
    {question}

    Answer:
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    # Retrieval QA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=compression_retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True,
    )

    logging.info("Advanced Legal Document RAG System initialized")
    return qa_chain


#  Ask questions loop
def ask_questions(qa_chain):
    logging.info("ðŸ“š Advanced Legal Document Q&A Ready!")
    logging.info("Type 'exit' to quit")

    while True:
        question = input("\nEnter your question: ").strip()
        if question.lower() in ["exit", "quit", "q"]:
            break

        try:
            # Retrieve
            logging.info("Retrieving documents...")
            result = qa_chain.invoke({"query": question})

            # Re-rank docs
            reranked_docs = rerank(question, result["source_documents"])

            # Display answer
            logging.info("\n Answer:")
            logging.info(result["result"])

            # Show top sources
            logging.info("\n Top Sources:")
            for i, doc in enumerate(reranked_docs[:3], 1):
                source = doc.metadata.get("source", "Unknown")
                logging.info(f"{i}. {source}")

        except Exception as e:
            logging.error(f" Error: {e}")



def main():
    qa_chain = initialize_rag_system()
    if qa_chain:
        ask_questions(qa_chain)
        
if __name__ == "__main__":
    main()
